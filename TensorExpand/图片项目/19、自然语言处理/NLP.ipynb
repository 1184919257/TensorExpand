{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.8-tf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  加载数据\n",
    "\n",
    "我们将使用由IMDB电影评论组成的50000个数据集。 Keras有一个内置的功能来下载一个类似的数据集（但显然是一半的大小）。 然而，Keras的版本已经将数据集中的文本转换为整数标记，这是使用自然语言工作的关键部分，本教程中还将演示这些语言，因此我们下载了实际的文本数据。\n",
    "\n",
    "注意：数据集为84 MB，将自动下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要将文件保存在其他目录中，请更改此项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb.data_dir = \"data/IMDB/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动下载并提取文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace is False and data exists, so doing nothing. Use replace==True to re-download the data.\n"
     ]
    }
   ],
   "source": [
    "imdb.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载训练和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, y_train = imdb.load_data(train=True)\n",
    "x_test_text, y_test = imdb.load_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  25000\n",
      "Test-set size:   25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-set size: \", len(x_train_text))\n",
    "print(\"Test-set size:  \", len(x_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将下面的一些用途组合成一个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = x_train_text + x_test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练集打印一个例子，看看数据看起来是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are not many movies around that have given me a feeling like Stardust did all throughout the course of the film. As magically fairy-tale-like as The Princess Bride, Stardust is most definitely the most wonderful fantasy spectacle of the 2000's as well as the 1990's. Exciting, hilarious and equipped with wonderful imagery as well as unforgettable characters, Michelle Pfeiffer and Robert DeNiro's especially, I challenge anyone to watch this movie without a smile. From the first ten minutes of the film you know perfectly well how it will end, but it is the journey and not the destination that enthralls the viewer from start to finish.<br /><br />Ten stars, and not a decimal less.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真正的“class”是电影评论的一种观点。 消极情绪为0.0，积极情绪为1.0。 在这种情况下，评论是积极的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标记生成器\n",
    "神经网络不能直接在文本字符串上工作，所以我们必须以某种方式对其进行转换。 这种转换有两个步骤，第一步称为“标记器”，它将单词转换为整数，并在输入到神经网络之前在数据集上完成。 第二步是神经网络本身的一个组成部分，被称为“嵌入”层，这在下面进一步描述。\n",
    "\n",
    "我们可能会指示标记器仅使用例如 数据集中最流行的10000个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标记器然后可以“拟合”到数据集。 这将扫描所有文本，并将其从不需要的字符（如标点符号）中除去，并将其转换为小写字符。 然后，令牌生成器将创建一个包含所有唯一字的词汇表以及用于访问数据的各种数据结构。\n",
    "\n",
    "请注意，我们在整个数据集上安装了标记器，以便从训练数据和测试数据中收集单词。 这是好的，因为我们只是建立一个词汇表，并希望它尽可能地完整。 实际的神经网络当然只能在训练集上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.93 s, sys: 16 ms, total: 6.95 s\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想使用整个词汇表，然后在上面设置num_words = None，那么它会自动设置为词汇大小。 （这是因为Keras的执行有些尴尬。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_words is None:\n",
    "    num_words = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以检查分词器收集的词汇。 这是按数据集中单词的出现次数排序的。 这些整数称为词索引或“标记”，因为它们唯一标识词汇表中的每个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'rabid\": 60582,\n",
       " 'elliptic': 67416,\n",
       " 'angelic': 16008,\n",
       " 'galactic': 23852,\n",
       " \"'breakin'\": 85947,\n",
       " 'suchen': 117451,\n",
       " 'leads\\x97gino': 82209,\n",
       " 'jewels': 11097,\n",
       " 'avp': 17115,\n",
       " 'wipes': 13631,\n",
       " 'roars': 19612,\n",
       " 'unscience': 97455,\n",
       " 'couching': 106044,\n",
       " 'smerdjakov': 119078,\n",
       " 'keeanu': 67172,\n",
       " 'rustling': 39529,\n",
       " \"five'\": 89293,\n",
       " 'parlay': 111397,\n",
       " 'unencumbered': 60276,\n",
       " 'perchance': 54104,\n",
       " 'council': 11242,\n",
       " 'wilting': 54069,\n",
       " 'beehives': 51778,\n",
       " 'wings': 5322,\n",
       " \"days''\": 108992,\n",
       " 'techy': 66799,\n",
       " 'tears': 1724,\n",
       " 'drudgery': 32425,\n",
       " 'hyperkinesis': 100706,\n",
       " 'irreligious': 76971,\n",
       " 'starker': 66712,\n",
       " 'slevin': 48799,\n",
       " 'public': 1071,\n",
       " 'pugninella': 118638,\n",
       " 'magowan': 62124,\n",
       " \"'shouldn't\": 90427,\n",
       " 'srathairn': 73837,\n",
       " 'kettlewell': 102492,\n",
       " 'dutton': 27686,\n",
       " 'fingerpainting': 120405,\n",
       " 'zurn': 110488,\n",
       " 'salò': 120180,\n",
       " 'uncontrollable': 15724,\n",
       " \"stoker's\": 42114,\n",
       " 'vive': 30165,\n",
       " 'almighty': 6635,\n",
       " 'abishek': 118762,\n",
       " 'straightforward': 5130,\n",
       " 'stingy': 28719,\n",
       " 'psych': 28476,\n",
       " \"moviegoers'\": 93569,\n",
       " \"blunt's\": 59064,\n",
       " 'shian': 95010,\n",
       " 'voilà': 31568,\n",
       " 'costco': 44664,\n",
       " 'attachs': 116497,\n",
       " 'vfc': 86256,\n",
       " '9as': 82782,\n",
       " 'joley': 102544,\n",
       " 'townies': 45987,\n",
       " 'upcoming': 7445,\n",
       " 'klaas': 42558,\n",
       " 'males': 6151,\n",
       " 'levels': 2048,\n",
       " 'expert': 3177,\n",
       " 'scalling': 102418,\n",
       " 'antebellum': 43895,\n",
       " 'na': 18984,\n",
       " 'tinian': 98833,\n",
       " 'preeti': 97059,\n",
       " 'gance': 72517,\n",
       " \"'extraordinary\": 47453,\n",
       " 'soni': 34163,\n",
       " 'fluffiness': 57921,\n",
       " '1887': 76763,\n",
       " 'wunderkinds': 78732,\n",
       " 'intermittedly': 90598,\n",
       " 'upright': 16174,\n",
       " \"nauvoo's\": 94695,\n",
       " 'winner': 2300,\n",
       " 'sheakspeare': 80766,\n",
       " 'foreshortened': 95275,\n",
       " 'intermitable': 86014,\n",
       " '211772166650071408': 99131,\n",
       " 'osmosis': 57606,\n",
       " 'follies': 24734,\n",
       " 'raza': 57598,\n",
       " 'recorders': 42073,\n",
       " 'aristotle': 32974,\n",
       " 'neophyte': 29504,\n",
       " 'waterboarded': 123685,\n",
       " 'tuckwiller': 97550,\n",
       " 'recipient': 21477,\n",
       " 'rockets': 10894,\n",
       " 'portico': 81671,\n",
       " 'maille': 87952,\n",
       " 'banx': 109520,\n",
       " 'liosa': 80423,\n",
       " 'ahalf': 115932,\n",
       " 'wei': 17652,\n",
       " 'guilted': 60923,\n",
       " 'grayscale': 81975,\n",
       " \"marco's\": 67641,\n",
       " 'lazer': 45899,\n",
       " 'oversentimental': 104428,\n",
       " \"zabalza's\": 94022,\n",
       " 'miniscule': 21055,\n",
       " 'firecrackers': 32663,\n",
       " 'pumpkinhead': 20689,\n",
       " \"dirt's\": 82218,\n",
       " 'c3p0': 88762,\n",
       " 'leonidas': 91632,\n",
       " 'bookwalter': 71091,\n",
       " 'companions': 9657,\n",
       " \"'intolerance\": 50304,\n",
       " \"crisp's\": 56055,\n",
       " 'drained': 9611,\n",
       " 'boobtube': 85753,\n",
       " 'kants': 60959,\n",
       " 'readjustment': 107653,\n",
       " 'sobeski': 71745,\n",
       " \"entwistle's\": 78970,\n",
       " 'shepherds': 43666,\n",
       " 'doria': 101918,\n",
       " 'rinne': 62310,\n",
       " 'demi': 9204,\n",
       " '0093638': 73517,\n",
       " 'sandstorms': 114633,\n",
       " 'inappropriate': 4554,\n",
       " 'puked': 32352,\n",
       " \"hum'\": 116433,\n",
       " \"rome's\": 55918,\n",
       " \"hobo'\": 121903,\n",
       " \"cannibal'\": 94910,\n",
       " 'selects': 36703,\n",
       " 'disproportionate': 38384,\n",
       " 'terrif': 108895,\n",
       " 'yella': 84725,\n",
       " 'unwinnable': 118468,\n",
       " 'nuel': 59028,\n",
       " 'surprise': 845,\n",
       " 'academian': 103640,\n",
       " 'gillmore': 64216,\n",
       " 'bloodhounds': 111788,\n",
       " 'wunderkind': 38194,\n",
       " 'sall': 74195,\n",
       " 'hve': 89413,\n",
       " 'lauret': 98824,\n",
       " 'degrassi': 50447,\n",
       " 'decorum': 34545,\n",
       " \"put's\": 50553,\n",
       " 'sinisterly': 66968,\n",
       " 'pianful': 84557,\n",
       " 'accept': 1727,\n",
       " 'deliberetly': 123932,\n",
       " 'clearest': 32924,\n",
       " 'painting': 3750,\n",
       " \"'posh'\": 91721,\n",
       " 'ecstatic': 15057,\n",
       " 'bankes': 71242,\n",
       " 'casella': 68630,\n",
       " 'jaybles': 100200,\n",
       " 'misfited': 102407,\n",
       " 'grody': 95136,\n",
       " 'possibilty': 118313,\n",
       " 'murali': 33135,\n",
       " 'poliwrath': 88519,\n",
       " 'amaze': 12638,\n",
       " 'pickpocketing': 66300,\n",
       " 'lagge': 97854,\n",
       " 'gound': 107797,\n",
       " 'convinced': 2668,\n",
       " \"rostotsky's\": 94653,\n",
       " \"wisdom'\": 67729,\n",
       " \"shave'\": 102802,\n",
       " 'opportunism': 44236,\n",
       " 'becasur': 111313,\n",
       " 'brisk': 12029,\n",
       " \"'huge\": 93648,\n",
       " \"'hearing'\": 80850,\n",
       " 'zombie': 1025,\n",
       " \"behind'\": 85444,\n",
       " 'recitation': 26724,\n",
       " 'pratfall': 28004,\n",
       " 'heinlein': 42914,\n",
       " 'cherio': 102463,\n",
       " 'intimite': 110607,\n",
       " 'organization': 6847,\n",
       " 'rigamortis': 114960,\n",
       " 'zoom': 8834,\n",
       " 'humoristic': 44009,\n",
       " 'ones': 674,\n",
       " 'deter': 18898,\n",
       " 'perms': 51502,\n",
       " \"parslow's\": 85154,\n",
       " \"'elephants'\": 79086,\n",
       " 'experimentaion': 104690,\n",
       " 'heartthrob': 23076,\n",
       " 'emeryville': 77017,\n",
       " 'lutheran': 47978,\n",
       " 'derogatorily': 117961,\n",
       " 'inaccurately': 38294,\n",
       " 'haggery': 103236,\n",
       " 'ignition': 37575,\n",
       " 'dignity': 3993,\n",
       " 'sunglasses': 13454,\n",
       " \"freakin'\": 17260,\n",
       " 'fodder': 6972,\n",
       " \"sapkowski's\": 33175,\n",
       " 'kilograms': 83575,\n",
       " 'killers': 2365,\n",
       " 'stockyards': 122926,\n",
       " 'abril': 40474,\n",
       " \"ulloa's\": 109838,\n",
       " \"fabulous'\": 107958,\n",
       " 'hoplite': 65990,\n",
       " 'vermin': 22382,\n",
       " 'couleur': 36116,\n",
       " \"mattei's\": 34666,\n",
       " 'lampedusan': 119897,\n",
       " 'caravana': 107154,\n",
       " 'claimer': 91992,\n",
       " 'hawked': 60895,\n",
       " '727': 118798,\n",
       " 'denman': 76275,\n",
       " 'totality': 34056,\n",
       " \"pond'\": 83555,\n",
       " 'quicker': 13600,\n",
       " 'kwei': 104393,\n",
       " 'señor': 58928,\n",
       " \"nolan'\": 87538,\n",
       " 'bertrand': 24733,\n",
       " 'smorgasbord': 47019,\n",
       " 'sequency': 90520,\n",
       " 'pest': 16031,\n",
       " 'distressing': 15519,\n",
       " 'harbou': 50794,\n",
       " 'streamlined': 33386,\n",
       " 'flirty': 28784,\n",
       " \"'video\": 27259,\n",
       " 'varying': 8815,\n",
       " 'hariett': 98082,\n",
       " '20061114': 111923,\n",
       " 'authorative': 98544,\n",
       " 'unpaved': 115178,\n",
       " 'etched': 16512,\n",
       " 'tamely': 98025,\n",
       " 'gwtw': 12849,\n",
       " 'ntire': 92752,\n",
       " 'gtho': 54632,\n",
       " 'trollop': 56165,\n",
       " 'tossers': 77954,\n",
       " \"'bedazzled'\": 43381,\n",
       " 'instigate': 46982,\n",
       " 'unblemished': 56466,\n",
       " 'flounces': 59777,\n",
       " 'coo': 26864,\n",
       " \"guinness's\": 40906,\n",
       " 'porsches': 57212,\n",
       " 'seachd': 45431,\n",
       " 'noy': 103930,\n",
       " 'biking': 40247,\n",
       " 'cinematrocity': 119462,\n",
       " '\\x96but': 87243,\n",
       " 'oozing': 17014,\n",
       " 'paintbrush': 44223,\n",
       " 'leaner': 45038,\n",
       " 'sprawling': 13595,\n",
       " 'obscessed': 81598,\n",
       " 'laud': 48863,\n",
       " 'wary': 14570,\n",
       " 'disrepute': 56976,\n",
       " 'jonny': 13701,\n",
       " 'overdoes': 20021,\n",
       " 'tasuiev': 40621,\n",
       " 'scabs': 69984,\n",
       " 'mobutu': 29304,\n",
       " 'viennale': 83998,\n",
       " 'championship': 5649,\n",
       " '«battlestar': 76880,\n",
       " 'zann': 92779,\n",
       " 'hydro': 26369,\n",
       " 'seltzer': 27181,\n",
       " 'djema': 103539,\n",
       " 'painlessly': 91166,\n",
       " 'domicile': 54004,\n",
       " 'interchange': 54733,\n",
       " \"'mad\": 42086,\n",
       " 'lenses': 15376,\n",
       " 'graciela': 92008,\n",
       " 'avast': 96782,\n",
       " 'salir': 113907,\n",
       " 'distraction': 6747,\n",
       " 'documentarians': 51578,\n",
       " \"station's\": 29756,\n",
       " 'disneylike': 74579,\n",
       " 'strongly': 2259,\n",
       " 'nourishes': 64393,\n",
       " 'neame': 42580,\n",
       " 'interconnects': 110610,\n",
       " 'bunta': 68968,\n",
       " 'broson': 57629,\n",
       " \"did\\x97i'm\": 121937,\n",
       " 'history\\x85': 76549,\n",
       " 'morvern': 22627,\n",
       " '«average»': 116236,\n",
       " \"'french\": 82813,\n",
       " 'welk': 52997,\n",
       " 'incurably': 50360,\n",
       " 'sustains': 18000,\n",
       " 'summering': 103293,\n",
       " \"hallowe'en\": 111357,\n",
       " 'assured': 5924,\n",
       " 'vardon': 17503,\n",
       " 'uomini': 39259,\n",
       " 'azar': 64419,\n",
       " 'maiko': 64105,\n",
       " 'eijanaika': 108882,\n",
       " 'formosa': 49997,\n",
       " 'burnsian': 95775,\n",
       " 'zaz': 39704,\n",
       " 'menalaus': 79409,\n",
       " \"'ju\": 65791,\n",
       " 'sittaford': 111249,\n",
       " 'clad': 5675,\n",
       " 'abetting': 90032,\n",
       " 'coronet': 102554,\n",
       " 'beecher': 33093,\n",
       " 'reel13': 95517,\n",
       " 'discriminates': 85052,\n",
       " \"veil's\": 62065,\n",
       " 'moonraker': 32170,\n",
       " 'mandates': 45442,\n",
       " 'preconceive': 71986,\n",
       " 'gores': 53496,\n",
       " \"ghose's\": 104311,\n",
       " 'imbb': 123421,\n",
       " 'aman': 31367,\n",
       " 'rollickingly': 106467,\n",
       " 'grueber': 112467,\n",
       " \"sutherland's\": 15226,\n",
       " 'fosters': 24703,\n",
       " 'belive': 27089,\n",
       " \"pakistani's\": 92012,\n",
       " 'impinge': 121120,\n",
       " 'choose\\x97see': 104132,\n",
       " 'margaritas': 40100,\n",
       " \"'upgrade'\": 75949,\n",
       " 'cupping': 96395,\n",
       " \"breckinridge's\": 96816,\n",
       " 'miscalculated': 44643,\n",
       " 'gristle': 68307,\n",
       " \"haley's\": 60159,\n",
       " 'venomously': 107117,\n",
       " \"blanks'\": 72031,\n",
       " 'trise': 54599,\n",
       " 'waxed': 38025,\n",
       " 'doobie': 54266,\n",
       " \"waynes'\": 84352,\n",
       " 'exam': 14330,\n",
       " 'mikio': 42130,\n",
       " 'edsel': 122575,\n",
       " 'entomology': 50738,\n",
       " 'isolationist': 39582,\n",
       " 'throwing': 2739,\n",
       " 'anakin': 17566,\n",
       " \"'depravity'\": 100131,\n",
       " 'fart': 7981,\n",
       " 'geek': 6622,\n",
       " 'polysyllabic': 77709,\n",
       " 'bets': 12202,\n",
       " 'binary': 38463,\n",
       " 'ealing': 10694,\n",
       " 'idiocyncracies': 109523,\n",
       " \"dbd's\": 81685,\n",
       " 'scruno': 118306,\n",
       " 'hayworth': 6206,\n",
       " 'cabel': 84421,\n",
       " 'wattis': 54432,\n",
       " 'kumarswamypillai': 103316,\n",
       " 'cosmologist': 102490,\n",
       " \"to've\": 103608,\n",
       " 'atomized': 86534,\n",
       " 'fathered': 28629,\n",
       " 'wowwwwww': 93360,\n",
       " 'icg': 65753,\n",
       " '2oo5': 83402,\n",
       " 'esqe': 43997,\n",
       " \"considering\\x97i've\": 118493,\n",
       " 'unstrained': 72853,\n",
       " 'below': 1793,\n",
       " 'beastmasters': 114543,\n",
       " 'adenoid': 27835,\n",
       " \"'date'\": 97401,\n",
       " 'shyama': 42184,\n",
       " 'word': 662,\n",
       " 'allegories': 36524,\n",
       " \"danes'\": 27926,\n",
       " 'contemplate': 11028,\n",
       " 'extremes': 10126,\n",
       " 'jogging': 20489,\n",
       " 'mdogg20': 92977,\n",
       " 'bradshaw': 20159,\n",
       " \"leland's\": 15410,\n",
       " 'linus': 38071,\n",
       " 'applecart': 88597,\n",
       " 'fyodor': 58587,\n",
       " 'wirtschaftswunder': 111481,\n",
       " 'lynchophiles': 70720,\n",
       " 'thrived': 33858,\n",
       " 'setbacks': 26835,\n",
       " \"mandel's\": 86120,\n",
       " 'dispatching': 27638,\n",
       " 'contents': 10673,\n",
       " 'carbines': 40032,\n",
       " 'thresa': 107650,\n",
       " \"minus's\": 95494,\n",
       " 'waded': 53766,\n",
       " 'hewlett': 20684,\n",
       " 'financiers': 26845,\n",
       " 'spanked': 32666,\n",
       " 'coordinate': 30627,\n",
       " 'facehuggers': 123708,\n",
       " \"bataille's\": 77665,\n",
       " 'tipper': 44507,\n",
       " 'stasi': 38069,\n",
       " 'balaun': 73439,\n",
       " \"'gladys'\": 111587,\n",
       " \"\\x91purity'\": 109913,\n",
       " 'mitropa': 84089,\n",
       " 'mittschnittservice': 104302,\n",
       " 'temptations': 20629,\n",
       " 'aahhhh': 121831,\n",
       " 'sydney': 5855,\n",
       " \"'epic'\": 110422,\n",
       " 'wip': 26334,\n",
       " 'hobbesian': 81556,\n",
       " 'instantaneously': 35112,\n",
       " 'bagdad': 14099,\n",
       " \"wit's\": 64668,\n",
       " 'pointer': 28406,\n",
       " 'privateer': 84371,\n",
       " 'strangling': 20605,\n",
       " 'underdeveloped': 7504,\n",
       " 'over\\x96the': 115375,\n",
       " \"isabella's\": 65874,\n",
       " 'wwe': 5502,\n",
       " 'naught': 25480,\n",
       " \"material's\": 41867,\n",
       " \"'magical\": 99406,\n",
       " 'pensylvannia': 86259,\n",
       " 'unbind': 80662,\n",
       " 'ubber': 88875,\n",
       " 'dio': 28553,\n",
       " \"mcgraw's\": 53341,\n",
       " 'outlandish': 8301,\n",
       " \"ajay's\": 86153,\n",
       " \"stalk'n\": 117599,\n",
       " 'convents': 80892,\n",
       " \"carlito's\": 14377,\n",
       " \"number'\": 82570,\n",
       " \"haw'\": 116793,\n",
       " 'drewbie': 86310,\n",
       " 'texaco': 114405,\n",
       " 'reece': 24580,\n",
       " 'remade': 6016,\n",
       " '1870s': 34977,\n",
       " \"tereza's\": 62273,\n",
       " 'granite': 41948,\n",
       " 'arsenical': 83241,\n",
       " 'bonafide': 36866,\n",
       " 'falfa': 122168,\n",
       " 'uprosing': 97136,\n",
       " 'squaws': 114888,\n",
       " 'ching': 12505,\n",
       " 'overarching': 33309,\n",
       " 'blondeau': 114424,\n",
       " 'gil': 12220,\n",
       " 'reevaluation': 120292,\n",
       " 'coastal': 12528,\n",
       " 'commercialization': 39716,\n",
       " 'pitman': 71730,\n",
       " 'guilfoyle': 35552,\n",
       " 'brokered': 78733,\n",
       " 'horrendous': 3470,\n",
       " 'drummer': 10046,\n",
       " 'abel': 16032,\n",
       " 'disused': 28239,\n",
       " 'ardentro': 111018,\n",
       " 'dumbed': 11115,\n",
       " 'choristers': 59807,\n",
       " 'grouth': 100899,\n",
       " 'unbelievers': 41512,\n",
       " \"goldeneye's\": 99234,\n",
       " 'cleanliness': 25857,\n",
       " 'rec': 24124,\n",
       " 'rejecting': 16657,\n",
       " 'dyslexic': 40301,\n",
       " \"117's\": 106128,\n",
       " 'summers': 20201,\n",
       " '1987': 5324,\n",
       " \"tadashi's\": 39404,\n",
       " 'heiko': 104890,\n",
       " '201': 62968,\n",
       " 'ueda': 68452,\n",
       " 'marauders': 60493,\n",
       " 'connolly': 13820,\n",
       " 'pece': 119509,\n",
       " 'disfigures': 93240,\n",
       " 'slinkys': 121054,\n",
       " \"gabrielle's\": 118789,\n",
       " 'mex': 70399,\n",
       " 'acceptable\\x85bloodline': 118623,\n",
       " 'vuosi': 38638,\n",
       " 'joyous': 11327,\n",
       " 'nommed': 101163,\n",
       " \"'lt'\": 94188,\n",
       " 'pseudos': 118641,\n",
       " 'day\\x85not': 111833,\n",
       " 'treckies': 100892,\n",
       " \"honkin'\": 65024,\n",
       " \"chadha's\": 74226,\n",
       " 'reparte': 112284,\n",
       " 'maleness': 103603,\n",
       " 'roadblocks': 54764,\n",
       " \"nunez's\": 57090,\n",
       " 'naïf': 48498,\n",
       " \"serial's\": 45415,\n",
       " \"'sweater\": 91163,\n",
       " 'evgar': 70567,\n",
       " 'cusses': 99151,\n",
       " 'lyubomir': 58486,\n",
       " 'proceeds': 4502,\n",
       " 'muthamittal': 33048,\n",
       " 'philanthropic': 45381,\n",
       " 'weawwy': 98441,\n",
       " 'egad': 117855,\n",
       " 'nonproportionally': 73447,\n",
       " 'informative': 6276,\n",
       " 'moderates': 70219,\n",
       " 'puchase': 79541,\n",
       " 'needlepoint': 64014,\n",
       " 'jeeder': 55926,\n",
       " 'deflector': 58497,\n",
       " 'gracelessly': 63243,\n",
       " 'maladroitness': 110613,\n",
       " 'murderer': 2754,\n",
       " 'misheard': 49685,\n",
       " 'column': 12903,\n",
       " 'ladders': 34648,\n",
       " \"'rita\": 121161,\n",
       " 'australain': 68183,\n",
       " 'kindles': 109951,\n",
       " 'abashidze': 85274,\n",
       " 'perfectionistic': 50979,\n",
       " 'montgomery': 5717,\n",
       " 'betters': 34112,\n",
       " 'overstaying': 107375,\n",
       " 'unleavened': 83766,\n",
       " 'filtered': 15667,\n",
       " 'effigies': 53227,\n",
       " 'settleling': 94706,\n",
       " 'reeducated': 102570,\n",
       " 'straithrain': 83882,\n",
       " 'exfriend': 99144,\n",
       " 'jubilant': 33490,\n",
       " 'colman': 10814,\n",
       " 'rouged': 59715,\n",
       " 'ocsar': 81501,\n",
       " 'destro': 118927,\n",
       " \"dress'\": 97820,\n",
       " 'anspach': 64623,\n",
       " 'forsaken': 14629,\n",
       " 'starewicz': 18715,\n",
       " 'bones': 4660,\n",
       " 'highjly': 78443,\n",
       " 'connative': 99161,\n",
       " 'unspooled': 112341,\n",
       " 'jesuit': 52707,\n",
       " 'mastrandrea': 106652,\n",
       " 'matthieu': 50953,\n",
       " '40mph': 65659,\n",
       " \"zelweger's\": 118302,\n",
       " 'pee': 8571,\n",
       " \"whately's\": 86353,\n",
       " 'brat': 6839,\n",
       " 'reflexions': 100753,\n",
       " 'choi': 14274,\n",
       " 'planned': 3933,\n",
       " 'eviller': 106194,\n",
       " 'zegers': 40931,\n",
       " 'renwick': 123595,\n",
       " 'denby': 100004,\n",
       " 'caterer': 41856,\n",
       " \"dentist's\": 25722,\n",
       " 'whitewolf': 70012,\n",
       " 'jar»': 79379,\n",
       " '1647': 121207,\n",
       " 'offscreen': 20395,\n",
       " 'maureen': 9434,\n",
       " 'wealth': 4574,\n",
       " 'bog': 12384,\n",
       " 'mayron': 79315,\n",
       " 'jon': 2940,\n",
       " '2020': 45413,\n",
       " 'severely': 4978,\n",
       " 'horrorfests': 108328,\n",
       " 'ignorant': 4657,\n",
       " 'overqualified': 58689,\n",
       " \"tetsuro's\": 60244,\n",
       " 'saintliness': 81832,\n",
       " 'temped': 120881,\n",
       " 'shroder': 107069,\n",
       " 'abusing': 13790,\n",
       " 'guerra': 60924,\n",
       " 'climb': 6957,\n",
       " 'hillside': 26186,\n",
       " 'candlelight': 27668,\n",
       " 'hool': 87083,\n",
       " 'cubicles': 103855,\n",
       " '1880': 43917,\n",
       " 'newberry': 74602,\n",
       " 'surveyors': 55807,\n",
       " \"whitlow's\": 90390,\n",
       " 'escape\\x85': 120855,\n",
       " 'bb': 21123,\n",
       " 'intrator': 116072,\n",
       " 'centaury': 86467,\n",
       " 'anesthesiologists': 97405,\n",
       " \"africa's\": 43166,\n",
       " 'georg': 31837,\n",
       " 'orkut': 103919,\n",
       " 'chandulal': 75157,\n",
       " 'trudeau': 41277,\n",
       " \"riot's\": 63005,\n",
       " 'reside': 16552,\n",
       " 'simple': 591,\n",
       " \"bard's\": 22326,\n",
       " 'dentisty': 92278,\n",
       " 'battlefields': 24705,\n",
       " '57d': 74886,\n",
       " 'stallone': 6763,\n",
       " 'jayne': 14303,\n",
       " 'groult': 80614,\n",
       " 'uneducated': 12568,\n",
       " 'asphyxiated': 78681,\n",
       " \"hogbottom's\": 89161,\n",
       " \"oppenheimer's\": 54433,\n",
       " 'redid': 62802,\n",
       " 'relegates': 64900,\n",
       " 'destructing': 43238,\n",
       " 'vittoria': 22682,\n",
       " 'mcdowall': 23187,\n",
       " 'lagers': 100492,\n",
       " 'plebeianism': 73236,\n",
       " 'kandahar': 66744,\n",
       " 'gaudenzi': 87695,\n",
       " 'lascher': 58001,\n",
       " 'hearald': 107676,\n",
       " 'jamaal': 100935,\n",
       " 'justifications': 36922,\n",
       " 'geilgud': 56259,\n",
       " 'modernistic': 43278,\n",
       " 'buck': 3360,\n",
       " 'inability': 5168,\n",
       " 'hahahahhaaa': 118351,\n",
       " \"cupid's\": 49968,\n",
       " 'jails': 30139,\n",
       " \"month's\": 30602,\n",
       " \"mukhsin's\": 42691,\n",
       " 'conservatism': 21802,\n",
       " 'swiftian': 121571,\n",
       " 'fear': 1130,\n",
       " \"infiniti's\": 83332,\n",
       " 'springer': 7736,\n",
       " 'prodding': 31927,\n",
       " \"l'appartement\": 26475,\n",
       " 'garments': 29986,\n",
       " 'finch': 11359,\n",
       " 'beetroot': 115805,\n",
       " \"'blackadder'\": 120463,\n",
       " 'kilpatrick': 27405,\n",
       " \"borer'\": 111516,\n",
       " 'revisioning': 95478,\n",
       " 'colder': 39281,\n",
       " 'ordo': 86382,\n",
       " 'lurhman': 112845,\n",
       " 'turmoil': 7436,\n",
       " 'mage': 80457,\n",
       " \"cort's\": 89688,\n",
       " 'withdrew': 56984,\n",
       " 'erle': 42390,\n",
       " 'tiff': 20290,\n",
       " \"gorman's\": 106525,\n",
       " \"'d\": 32649,\n",
       " 'newsletter': 40835,\n",
       " 'stepanov': 104200,\n",
       " 'chatty': 23311,\n",
       " \"astrid's\": 119152,\n",
       " \"view'\": 41076,\n",
       " '1891': 42336,\n",
       " 'bhandarkar': 20880,\n",
       " 'breakfasts': 81987,\n",
       " \"erroll's\": 51882,\n",
       " 'brenner': 55198,\n",
       " \"knightley's\": 27057,\n",
       " 'itz': 88291,\n",
       " 'kick': 1923,\n",
       " 'experimental': 4644,\n",
       " 'tsar': 42192,\n",
       " 'lipnicki': 91863,\n",
       " \"limbaugh's\": 93341,\n",
       " 'colourful': 9756,\n",
       " 'undervalued': 23805,\n",
       " 'expeditiously': 58539,\n",
       " 'welfare': 12692,\n",
       " 'absoloutely': 106843,\n",
       " 'monday': 9139,\n",
       " 'muddied': 30711,\n",
       " 'unplugged': 79657,\n",
       " 'striaght': 115237,\n",
       " 'complicated': 2769,\n",
       " 'acropolis': 60762,\n",
       " 'chocking': 62480,\n",
       " 'safer': 16551,\n",
       " \"allows'\": 79224,\n",
       " 'rightful': 15770,\n",
       " 'ughhh': 119927,\n",
       " 'jezek': 104094,\n",
       " 'azaleigh': 114617,\n",
       " 'reconnects': 44544,\n",
       " '1938': 8718,\n",
       " 'feb': 31957,\n",
       " 'spiritless': 41491,\n",
       " 'vannoord': 56453,\n",
       " 'chez': 26754,\n",
       " 'iliad': 27921,\n",
       " 'changi': 14241,\n",
       " 'squid': 18284,\n",
       " \"'aankhen'\": 60368,\n",
       " 'sweetening': 114127,\n",
       " 'choca': 87209,\n",
       " \"tomb's\": 120577,\n",
       " 'paintings': 5715,\n",
       " 'cdc': 70107,\n",
       " 'deified': 79455,\n",
       " 'hesitance': 113657,\n",
       " \"members'\": 36696,\n",
       " 'benteen': 111369,\n",
       " '346': 123695,\n",
       " 'ralli': 30080,\n",
       " 'unsympathetic': 7097,\n",
       " 'ajeeb': 48889,\n",
       " 'wishful': 20762,\n",
       " 'skull': 5876,\n",
       " 'heartbeats': 67205,\n",
       " 'fedar': 78564,\n",
       " 'independentcritics': 77111,\n",
       " 'supermen': 43200,\n",
       " 'chaplins': 62115,\n",
       " 'drssing': 98299,\n",
       " 'você': 76002,\n",
       " 'breakaway': 97496,\n",
       " 'acclimation': 94475,\n",
       " 'osaka': 53934,\n",
       " 'belair': 99218,\n",
       " 'scared': 1822,\n",
       " 'hendrix': 15257,\n",
       " 'jenova': 68329,\n",
       " 'rawks': 78678,\n",
       " 'quitte': 48102,\n",
       " \"'isms'\": 88813,\n",
       " \"'story'of\": 86160,\n",
       " \"capshaw's\": 38038,\n",
       " 'crombie': 122243,\n",
       " 'gazer': 55029,\n",
       " 'woodenly': 37930,\n",
       " 'perks': 18735,\n",
       " 'superstitions': 26894,\n",
       " 'fantine': 52930,\n",
       " 'danilo': 29305,\n",
       " 'misra': 66223,\n",
       " 'asawari': 113551,\n",
       " 'pads': 25875,\n",
       " 'kroft': 80176,\n",
       " 'ruta': 96005,\n",
       " 'toklas': 91510,\n",
       " \"darkman's\": 63071,\n",
       " 'camerashots': 76075,\n",
       " 'literates': 63790,\n",
       " 'lynchian': 17739,\n",
       " 'conundrums': 38919,\n",
       " 'successful': 1139,\n",
       " 'condescend': 80918,\n",
       " 'knotty': 67712,\n",
       " 'aromatic': 49765,\n",
       " \"marcy's\": 94975,\n",
       " 'nina': 7366,\n",
       " 'beatings': 17691,\n",
       " \"polish's\": 55383,\n",
       " 'dhoom': 21089,\n",
       " 'inaccessible': 25522,\n",
       " 'parekh': 49380,\n",
       " 'rourke': 5757,\n",
       " \"leto's\": 97191,\n",
       " 'cytown': 64379,\n",
       " 'culea': 66821,\n",
       " 'adjuncts': 76941,\n",
       " 'thumbnail': 56801,\n",
       " \"'cops\": 95829,\n",
       " 'supposes': 38096,\n",
       " 'headgames': 101712,\n",
       " \"sonnenschein'\": 87165,\n",
       " 'revolutions': 20982,\n",
       " 'wulfsohn': 71709,\n",
       " 'loke': 121371,\n",
       " 'ock': 33680,\n",
       " 'volvo': 41899,\n",
       " 'prem': 10111,\n",
       " 'tuscan': 27105,\n",
       " 'depths': 5971,\n",
       " 'vegetating': 75581,\n",
       " 'movieplex': 115697,\n",
       " 'loris': 88880,\n",
       " 'nicklodeon': 97095,\n",
       " \"'live\": 37680,\n",
       " 'modify': 34232,\n",
       " 'tigellinus': 70401,\n",
       " 'paulyn': 71207,\n",
       " 'sacrficing': 76506,\n",
       " 'shoving': 15290,\n",
       " 'broadway': 2438,\n",
       " 'booz': 77865,\n",
       " \"stable's\": 109276,\n",
       " 'rain': 2788,\n",
       " 'australia': 3179,\n",
       " 'tavern': 17637,\n",
       " 'misspent': 44345,\n",
       " 'buzzards': 54292,\n",
       " 'bindingly': 117651,\n",
       " \"shenk's\": 88376,\n",
       " 'trench': 14590,\n",
       " 'befalling': 55327,\n",
       " \"'hoovervilles'\": 82384,\n",
       " 'dassin': 11152,\n",
       " 'yee': 41099,\n",
       " 'pfalz': 118859,\n",
       " '3000': 4936,\n",
       " \"iris's\": 70439,\n",
       " 'athmosphere': 93091,\n",
       " 'televangelism': 92364,\n",
       " 'specialization': 61233,\n",
       " 'mão': 60504,\n",
       " 'testa': 41754,\n",
       " 'conover': 72021,\n",
       " 'prophecies': 21227,\n",
       " 'premarital': 41982,\n",
       " 'unmatchable': 49357,\n",
       " 'musashi': 58582,\n",
       " 'gullable': 68660,\n",
       " 'gomorrah': 66041,\n",
       " \"molina's\": 37978,\n",
       " 'pontins': 70033,\n",
       " 'cooke': 28656,\n",
       " 'typewriter': 29106,\n",
       " 'hit\\x85': 100861,\n",
       " 'seagal': 2496,\n",
       " 'cockpit': 14033,\n",
       " 'lilliputians': 68569,\n",
       " \"investigation'\": 79039,\n",
       " \"amelio's\": 38565,\n",
       " 'cloths': 29215,\n",
       " 'ariana': 34765,\n",
       " 'bulldozing': 47633,\n",
       " \"marquez'\": 77282,\n",
       " 'variants': 79369,\n",
       " 'faun': 61498,\n",
       " \"marsh's\": 69041,\n",
       " 'ragazza': 32231,\n",
       " 'handcuffs': 19703,\n",
       " 'launched': 8904,\n",
       " \"servant's\": 63411,\n",
       " 'wallece': 75548,\n",
       " 'ditziness': 109409,\n",
       " 'vitto': 116003,\n",
       " 'hemo': 30626,\n",
       " '372': 85719,\n",
       " 'brekinridge': 98334,\n",
       " '454': 89254,\n",
       " 'potere': 47832,\n",
       " 'multilevel': 82877,\n",
       " 'stat': 53453,\n",
       " 'tropic': 60071,\n",
       " 'lagoon': 14470,\n",
       " 'overage': 44650,\n",
       " 'director\\x85': 96337,\n",
       " \"davidtz's\": 59983,\n",
       " 'cruellest': 60481,\n",
       " 'trips': 9126,\n",
       " 'expanses': 45305,\n",
       " 'patently': 14233,\n",
       " 'notise': 96786,\n",
       " \"saif''s\": 87147,\n",
       " 'giggling': 11384,\n",
       " \"donut's\": 108370,\n",
       " 'jun': 23918,\n",
       " 'arachnid': 63074,\n",
       " 'sedatives': 62391,\n",
       " \"jeff's\": 25990,\n",
       " 'disintegrated': 33702,\n",
       " \"kaylee's\": 103665,\n",
       " 'paley': 50833,\n",
       " 'accusers': 62394,\n",
       " 'sssr': 72702,\n",
       " 'mimes': 24787,\n",
       " 'oversimplification': 64392,\n",
       " 'epigraph': 71327,\n",
       " 'breakin': 38749,\n",
       " 'roast': 22560,\n",
       " 'jewel': 5181,\n",
       " 'homelessness': 30724,\n",
       " 'lollabrigida': 60085,\n",
       " 'booi': 98554,\n",
       " 'axiom': 46247,\n",
       " \"admirer's\": 64712,\n",
       " 'smörgåsbord': 62251,\n",
       " 'demonous': 87266,\n",
       " 'neorealist': 33517,\n",
       " 'fosco': 55164,\n",
       " 'luberon': 104087,\n",
       " 'chani': 116041,\n",
       " \"10's\": 23450,\n",
       " 'acczent': 122567,\n",
       " 'fenwick': 40886,\n",
       " \"sharon's\": 97837,\n",
       " 'dominates': 11051,\n",
       " 'yorker': 14765,\n",
       " '67th': 60491,\n",
       " 'chippettes': 103330,\n",
       " 'twirl': 40727,\n",
       " \"frazier's\": 98702,\n",
       " 'lamposts': 84137,\n",
       " 'miramax': 15750,\n",
       " 'catastrophically': 46847,\n",
       " 'lugacy': 61878,\n",
       " 'aiden': 14323,\n",
       " 'asshats': 78421,\n",
       " 'recon': 32578,\n",
       " 'kodachrome': 115005,\n",
       " \"grutter's\": 69433,\n",
       " \"'l'\": 87863,\n",
       " 'shalit': 106257,\n",
       " 'shabbiness': 50521,\n",
       " 'tenebra': 97403,\n",
       " 'umcomfortable': 97495,\n",
       " 'chats': 25793,\n",
       " \"slash'\": 90862,\n",
       " 'wearily': 45378,\n",
       " 'spilchuk': 71496,\n",
       " 'narcissistic': 10056,\n",
       " \"culture's\": 34538,\n",
       " 'fireman': 19463,\n",
       " 'steaming': 8369,\n",
       " 'pilgrims': 43531,\n",
       " 'nob': 62104,\n",
       " \"sum's\": 90363,\n",
       " \"joyce's\": 16054,\n",
       " 'wexford': 59577,\n",
       " 'mp': 28785,\n",
       " 'laurents': 97934,\n",
       " 'monsey': 93967,\n",
       " 'lochley': 57439,\n",
       " 'harpooner': 89841,\n",
       " 'pettily': 91819,\n",
       " 'if\\xa0you': 78091,\n",
       " \"predator'\": 70295,\n",
       " 'heare': 108121,\n",
       " 'shreveport': 94264,\n",
       " \"cowgirl's\": 78737,\n",
       " 'sighed': 33921,\n",
       " 'volkswagen': 37654,\n",
       " \"madhvi'\": 78111,\n",
       " 'arrogance': 8193,\n",
       " 'hassassin': 71077,\n",
       " 'trumpeter': 30259,\n",
       " 'trios': 48334,\n",
       " 'ro': 53684,\n",
       " \"tap's\": 84252,\n",
       " 'gilkyson': 70499,\n",
       " 'deific': 110852,\n",
       " 'simpley': 60463,\n",
       " \"pastor's\": 45766,\n",
       " 'keying': 59266,\n",
       " 'servitude': 24911,\n",
       " 'seeping': 39170,\n",
       " 'triple': 6310,\n",
       " 'chastising': 41635,\n",
       " \"'die\": 33794,\n",
       " 'jeffry': 87923,\n",
       " \"'cheese'\": 74561,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以使用标记器将训练集中的所有文本转换为这些标记的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，以下是训练集中的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are not many movies around that have given me a feeling like Stardust did all throughout the course of the film. As magically fairy-tale-like as The Princess Bride, Stardust is most definitely the most wonderful fantasy spectacle of the 2000's as well as the 1990's. Exciting, hilarious and equipped with wonderful imagery as well as unforgettable characters, Michelle Pfeiffer and Robert DeNiro's especially, I challenge anyone to watch this movie without a smile. From the first ten minutes of the film you know perfectly well how it will end, but it is the journey and not the destination that enthralls the viewer from start to finish.<br /><br />Ten stars, and not a decimal less.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该文本对应于以下令牌列表：文本转成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  46,   23,   21,  106,   97,  183,   12,   25,  358,   68,    3,\n",
       "        558,   37,  115,   29,  474,    1,  265,    4,    1,   19,   14,\n",
       "       7658, 3065,  793,   37,   14,    1, 2761, 3655,    6,   88,  406,\n",
       "          1,   88,  393, 1029, 6088,    4,    1,   14,   69,   14,    1,\n",
       "       8602, 1141,  578,    2,   16,  393, 2657,   14,   69,   14, 3202,\n",
       "        102, 2917, 6187,    2,  610,  261,   10, 3029,  250,    5,  103,\n",
       "         11,   17,  208,    3, 1861,   36,    1,   86,  713,  228,    4,\n",
       "          1,   19,   22,  118,  922,   69,   85,    9,   80,  127,   18,\n",
       "          9,    6,    1, 1261,    2,   21,    1, 6303,   12,    1,  536,\n",
       "         36,  375,    5, 1391,    7,    7,  713,  405,    2,   21,    3,\n",
       "        340])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们还需要将测试集中的文本转换为标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 填充和截断数据\n",
    "递归神经网络可以将任意长度的序列作为输入，但为了使用整批数据，序列需要具有相同的长度。 有两种方法可以实现这一点：（A）要么我们确保整个数据集中的所有序列具有相同的长度，要么（B）我们编写一个自定义数据生成器，以确保序列在每个批次中具有相同的长度。\n",
    "\n",
    "解决方案（A）更简单，但如果我们使用数据集中最长序列的长度，那么我们浪费了大量内存。 这对于大型数据集尤为重要。\n",
    "\n",
    "所以为了做出妥协，我们将使用涵盖数据集中大部分序列的序列长度，然后我们将截断较长的序列并填充较短的序列。\n",
    "\n",
    "首先我们计算数据集中所有序列中的令牌数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列中的平均令牌数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.27716000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列中令牌的最大数量为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们允许的令牌的最大数量设置为平均值加2个标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这涵盖了约95％的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94532000000000005"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当填充或截断具有不同长度的序列时，我们需要确定是否要填充或截断“pre”或“post”。 如果序列被截断，则意味着序列的一部分被简单地丢弃。 如果序列被填充，则表示将零添加到序列中。\n",
    "\n",
    "所以'pre'或'post'的选择可能很重要，因为它决定了我们是否在截断时丢弃了序列的第一部分或最后一部分，并决定了在填充时是否将序列的零或开头添加到序列的末尾。 这可能会混淆循环神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经将训练集转换成了一个具有这种形状的整数（令牌）的大矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集的矩阵具有相同的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，我们在上面有以下令牌序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  46,   23,   21,  106,   97,  183,   12,   25,  358,   68,    3,\n",
       "        558,   37,  115,   29,  474,    1,  265,    4,    1,   19,   14,\n",
       "       7658, 3065,  793,   37,   14,    1, 2761, 3655,    6,   88,  406,\n",
       "          1,   88,  393, 1029, 6088,    4,    1,   14,   69,   14,    1,\n",
       "       8602, 1141,  578,    2,   16,  393, 2657,   14,   69,   14, 3202,\n",
       "        102, 2917, 6187,    2,  610,  261,   10, 3029,  250,    5,  103,\n",
       "         11,   17,  208,    3, 1861,   36,    1,   86,  713,  228,    4,\n",
       "          1,   19,   22,  118,  922,   69,   85,    9,   80,  127,   18,\n",
       "          9,    6,    1, 1261,    2,   21,    1, 6303,   12,    1,  536,\n",
       "         36,  375,    5, 1391,    7,    7,  713,  405,    2,   21,    3,\n",
       "        340])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这只是填充以创建以下序列。 请注意，当它被输入到递归神经网络时，它首先输入很多零。 如果我们填充了'post'，那么它会首先输入整数标记，然后输入很多零。 这可能会混淆循环神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,   46,   23,   21,  106,   97,  183,   12,\n",
       "         25,  358,   68,    3,  558,   37,  115,   29,  474,    1,  265,\n",
       "          4,    1,   19,   14, 7658, 3065,  793,   37,   14,    1, 2761,\n",
       "       3655,    6,   88,  406,    1,   88,  393, 1029, 6088,    4,    1,\n",
       "         14,   69,   14,    1, 8602, 1141,  578,    2,   16,  393, 2657,\n",
       "         14,   69,   14, 3202,  102, 2917, 6187,    2,  610,  261,   10,\n",
       "       3029,  250,    5,  103,   11,   17,  208,    3, 1861,   36,    1,\n",
       "         86,  713,  228,    4,    1,   19,   22,  118,  922,   69,   85,\n",
       "          9,   80,  127,   18,    9,    6,    1, 1261,    2,   21,    1,\n",
       "       6303,   12,    1,  536,   36,  375,    5, 1391,    7,    7,  713,\n",
       "        405,    2,   21,    3,  340], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer反向映射\n",
    "出于某种奇怪的原因，Keras实现一个标记器似乎没有将整数标记反向映射回单词，这需要从令牌列表重建文本字符串。 所以我们在这里做这个映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辅助函数用于将令牌列表转换回单词串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，这是来自数据集的原始文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are not many movies around that have given me a feeling like Stardust did all throughout the course of the film. As magically fairy-tale-like as The Princess Bride, Stardust is most definitely the most wonderful fantasy spectacle of the 2000's as well as the 1990's. Exciting, hilarious and equipped with wonderful imagery as well as unforgettable characters, Michelle Pfeiffer and Robert DeNiro's especially, I challenge anyone to watch this movie without a smile. From the first ten minutes of the film you know perfectly well how it will end, but it is the journey and not the destination that enthralls the viewer from start to finish.<br /><br />Ten stars, and not a decimal less.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过将标记列表转换回单词来重新创建除标点符号和其他符号之外的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"there are not many movies around that have given me a feeling like did all throughout the course of the film as magically fairy tale like as the princess bride is most definitely the most wonderful fantasy spectacle of the as well as the 1990's exciting hilarious and with wonderful imagery as well as unforgettable characters michelle pfeiffer and robert especially i challenge anyone to watch this movie without a smile from the first ten minutes of the film you know perfectly well how it will end but it is the journey and not the destination that the viewer from start to finish br br ten stars and not a less\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建循环神经网络\n",
    "我们现在准备创建循环神经网络（RNN）。 由于其简单性，我们将使用Keras API。 有关Keras的教程，请参见教程＃03-C。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN中的第一层是所谓的嵌入层，其将每个整数标记转换为值向量。 这是必要的，因为对于10000字的词汇表，整数标记可以取0到10000之间的值。 RNN无法处理如此广泛的数值。 嵌入层被训练为RNN的一部分，并将学习将具有相似语义含义的单词映射到相似的嵌入向量，如下面将进一步示出的那样。\n",
    "\n",
    "首先我们为每个整数标记定义嵌入向量的大小。 在这种情况下，我们将其设置为8，以便每个整数标记都将转换为长度为8的矢量。嵌入矢量的值通常大致落在-1.0和1.0之间，尽管它们可能会略微超过这些值。\n",
    "\n",
    "嵌入向量的大小通常在100-300之间选择，但对于情感分析来说，它似乎可以很好地工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入层还需要知道词汇表（num_words）中的单词数量和填充的标记序列（max_tokens）的长度。 我们也给这个图层一个名字，因为我们需要在下面进一步检索它的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以将第一个门控循环单元（GRU）添加到网络中。 这将有16个输出。 因为我们将在此之后添加第二个GRU，所以我们需要返回数据序列，因为下一个GRU需要将序列作为其输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=16, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这增加了具有8个输出单元的第二个GRU。 接下来是另一个GRU，所以它也必须返回序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=8, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这增加了第三个也是最后一个GRU，具有4个输出单位。 紧接着是一个密集层，所以它应该只给出GRU的最终输出，而不是一个完整的输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加一个完全连接/密集层，计算0.0到1.0之间的值作为分类输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用具有给定学习率的Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译Keras模型，以便可以进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 16)          1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 8)           600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环神经网络\n",
    "我们现在可以训练模型。 请注意，我们正在使用填充序列的数据集。 我们使用5％的训练集作为一个小的验证集，所以我们对这个模型是否泛化很好或者它是否适合训练集有一个粗略的认识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 435s - loss: 0.6597 - acc: 0.5776 - val_loss: 0.3170 - val_acc: 0.9216\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 435s - loss: 0.4068 - acc: 0.8248 - val_loss: 0.2846 - val_acc: 0.8984\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 410s - loss: 0.2943 - acc: 0.8852 - val_loss: 0.3789 - val_acc: 0.8504\n",
      "CPU times: user 30min 26s, sys: 1min 28s, total: 31min 54s\n",
      "Wall time: 21min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f0163183978>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性能测试集\n",
    "现在模型已经过训练，我们可以计算它在测试集上的分类精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 168s   \n",
      "CPU times: user 2min 51s, sys: 0 ns, total: 2min 51s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.74%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 错误分类的文本示例\n",
    "为了显示错误分类文本的例子，我们首先计算测试集中前1000个文本的预测情绪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.88 s, sys: 0 ns, total: 6.88 s\n",
      "Wall time: 6.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = model.predict(x=x_test_pad[0:1000])\n",
    "y_pred = y_pred.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些预测数字在0.0到1.0之间。 我们使用截止点/阈值，并且说0.5以上的所有值取1.0，所有低于0.5的值取0.0。 这给了我们一个0.0或1.0的预测“类”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集中前1000个文本的真实“类”是需要比较的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_true = np.array(y_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以通过比较这两个数组的所有“类”来获得所有被错误分类的文本的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用的1000篇文章中，有多少是错误分类的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看第一个错误分类的文本。 我们会多次使用它的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = incorrect[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误分类的文字是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When Liv Ullman\\'s character says, \"I feel like I\\'m in someone else\\'s dream and they\\'re going to be ashamed when they wake up,\" she is referring not only to being an unwilling player in society\\'s war games, she is referring to being an ignorant participant in life itself. At the film\\'s end, when she says that she had a dream that she had a child and she was trying to take care of it, but she forgot something else, the implication is that she has forgotten what she has learned in the war she\\'s just survived, that like her own mother before her, she will be unable to pass on any vital lessons to her own child. And, therefore, the cycle of the shame of ignorance will continue...ad infinitum...'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = x_test_text[idx]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些是文本预测的和真实的类别："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12626585"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_true[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新数据\n",
    "让我们尝试分类我们组成的新文本。 其中一些是显而易见的，而另一些则使用否定和讽刺来试图混淆模型，将文本分类错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先将这些文本转换为整数标记数组，因为这是模型所需要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将不同长度的文本输入到模型中，我们还需要填充和截断它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 544)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "tokens_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以使用训练好的模型来预测这些文本的情绪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81724364],\n",
       "       [ 0.67148209],\n",
       "       [ 0.42932153],\n",
       "       [ 0.58877438],\n",
       "       [ 0.39098719],\n",
       "       [ 0.14605264],\n",
       "       [ 0.63294291],\n",
       "       [ 0.20740262]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接近0.0的数值意味着负面情绪，接近1.0的数值意味着积极的情绪。 每次训练模型时，这些数字都会有所不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 嵌入\n",
    "该模型不能直接在整数标记上工作，因为它们的整数值可能介于0和词汇表中的词数之间，例如， 10000.因此，我们需要将整数标记转换为大致介于-1.0和1.0之间的值的向量，这些值可用作神经网络的输入。\n",
    "\n",
    "从整数标记到实值矢量的映射也称为“嵌入”。 它基本上只是一个矩阵，每行包含单个标记的向量映射。 这意味着我们可以通过简单地使用该标记作为矩阵的索引来快速查找每个整数标记的映射。 在训练过程中，嵌入与模型的其余部分一起学习。\n",
    "\n",
    "理想情况下，嵌入会学习一个映射，其中意义上相似的单词也具有相似的嵌入值。 让我们来调查这是否发生在这里。\n",
    "\n",
    "首先，我们需要从模型中获取嵌入层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embedding = model.get_layer('layer_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以获得嵌入层完成映射的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding = layer_embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，权重实际上只是一个矩阵，其中词汇表中的词数乘以每个嵌入的向量长度。 这是因为它基本上只是一个查找矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们得到单词'good'的整数标记，它只是词汇表中的一个索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_good = tokenizer.word_index['good']\n",
    "token_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们也得到单词'great'的整数标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_great = tokenizer.word_index['great']\n",
    "token_great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些integertokens可能会很远，并将取决于数据集中这些单词的频率。\n",
    "\n",
    "现在让我们比较一下“good”和“great”两个词的矢量嵌入。 其中几个值是相似的，但有些值是完全不同的。 请注意，每次训练模型时，这些值都会改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66052264,  0.5165453 ,  0.53161222,  0.49544814,  0.42752418,\n",
       "        1.19518912,  0.66276532,  0.16886722], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30738267,  1.4632163 ,  0.41399351,  0.87894005,  0.25113523,\n",
       "        1.25763977,  0.94943362,  0.04790188], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_great]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们可以比较“bad”和“horrible”两个词的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_bad = tokenizer.word_index['bad']\n",
    "token_horrible = tokenizer.word_index['horrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62014925, -0.23285469,  0.77010238, -0.19315891,  1.21860778,\n",
       "       -0.01226851,  0.52136087,  0.51562059], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98533154, -0.25480568,  0.80838662,  0.36212307,  0.54833591,\n",
       "       -0.29700345,  0.08748548,  0.53162032], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_horrible]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 排序词\n",
    "我们还可以根据嵌入空间中的“相似性”对词汇表中的所有单词进行排序。 我们想要看看具有相似嵌入向量的词是否也具有相似的含义。\n",
    "\n",
    "嵌入向量的相似性可以通过不同的度量来度量，例如， 欧几里德距离或余弦距离。\n",
    "\n",
    "我们有一个帮助函数用于计算这些距离并按排序顺序打印文字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],\n",
    "                      metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "    \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以根据向量嵌入来打印与“great”这个词相距甚远的单词。 请注意，这些可能会在您每次训练模型时改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'great':\n",
      "-0.000 - great\n",
      "0.009 - tremendous\n",
      "0.009 - tank\n",
      "0.019 - sunset\n",
      "0.022 - hellraiser\n",
      "0.025 - democracy\n",
      "0.026 - 1934\n",
      "0.027 - royal\n",
      "0.029 - row\n",
      "0.030 - letterman\n",
      "...\n",
      "0.833 - travesty\n",
      "0.839 - worse\n",
      "0.848 - pointless\n",
      "0.880 - horrible\n",
      "0.886 - mess\n",
      "0.889 - avoid\n",
      "0.894 - drags\n",
      "0.963 - terrible\n",
      "1.002 - worst\n",
      "1.098 - waste\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('great', metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们可以根据向量嵌入来打印离“worst”一词最近和最远的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'worst':\n",
      "0.000 - worst\n",
      "0.051 - incoherent\n",
      "0.076 - pathetic\n",
      "0.097 - wasted\n",
      "0.098 - ridiculous\n",
      "0.099 - rubbish\n",
      "0.106 - poor\n",
      "0.107 - terrible\n",
      "0.107 - drags\n",
      "0.108 - mess\n",
      "...\n",
      "1.189 - maintained\n",
      "1.189 - gruff\n",
      "1.200 - facing\n",
      "1.201 - 'a\n",
      "1.210 - 7\n",
      "1.219 - superb\n",
      "1.303 - heart\n",
      "1.345 - worlds\n",
      "1.398 - favorite\n",
      "1.501 - excellent\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('worst', metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "本教程展示了使用具有整数标记和嵌入层的递归神经网络进行自然语言处理（NLP）的基本方法。 这被用来对IMDB的电影评论进行情感分析。 如果正确选择超参数，它工作得相当好。 但重要的是要理解这不是人类对文本的理解。 该系统对文本没有任何真正的理解。 这只是一种巧妙的模式识别方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习\n",
    "这些是一些可能有助于提高TensorFlow技能的练习建议。获得TensorFlow的实践经验对于学习如何正确使用它非常重要。\n",
    "进行任何更改之前，您可能需要备份此笔记本。\n",
    "\n",
    "- 运行更多的培训时代。它会提高性能吗？\n",
    "- 如果您的模型适合训练数据，请尝试在GRU中使用丢失图层和丢失。\n",
    "- 增加或减少词汇表中的单词数量。这在Tokenizer被初始化时完成。它会影响性能吗？\n",
    "- 将嵌入向量的大小增加到例如200.它会影响性能吗？\n",
    "- 尝试改变递归神经网络的所有不同的超参数。\n",
    "- 使用教程＃19中的贝叶斯优化来找到超参数的最佳选择。\n",
    "- 使用'post'填充和截断pad_sequences（）。它会影响性能吗？\n",
    "- 使用单个字符而不是标记化词作为词汇表。然后，您可以对每个字符使用单独编码的矢量，而不是使用嵌入图层。\n",
    "- 使用model.fit_generator（）而不是model.fit（）并创建自己的数据生成器，该数据生成器使用x_train_tokens的随机子集创建一批数据。序列必须填充，以便它们全部匹配最长序列的长度。\n",
    "- 向朋友解释程序是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
