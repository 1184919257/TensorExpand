参考：

- [TensorFlow Tutorial #20 Natural Language Processing](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb)
- https://github.com/Hvass-Labs/TensorFlow-Tutorials


----------
# 介绍
本教程是关于自然语言处理（NLP）的一种基本形式，称为情感分析，我们将尝试将电影评论分类为正面或负面。
考虑一个简单的例子：“这部电影不太好。”本文最后以“非常好”这个词表示非常积极的情绪，但由于它之前是“not”这个词，因此它被否定，所以文本应该被分类为具有负面情绪。我们如何教一个神经网络做这种分类？

另一个问题是神经网络无法直接处理文本数据，因此我们需要将文本转换为与神经网络兼容的数字。

还有一个问题是文本可能是任意长的。我们在前面的教程中使用过的神经网络使用固定的数据形状 - 除了数据的第一个维度因批处理大小而异。现在我们需要一种可以处理短文本和长文本序列的神经网络。

一般来说，您应该熟悉TensorFlow和Keras，参见教程＃01和＃03-C。

# 流程图
为了解决这个问题，我们需要几个处理步骤。 首先，我们需要将原始文本字转换为所谓的令牌，它们是整数值。 这些令牌实际上只是整个词汇表的**索引**。 然后我们将这些整数标记转化为所谓的**嵌入值**，这些嵌入值是实值向量，其映射将与神经网络一起训练，从而将具有相似含义的词映射到相似的嵌入向量。 然后我们将这些嵌入向量输入到一个递归神经网络中，该网络可以将任意长度的序列作为输入并输出它在输入中看到的内容的总结。 然后使用Sigmoid函数将此输出压扁，从而得出0.0和1.0之间的值，其中0.0表示负面情绪，1.0表示积极情绪。 整个过程使我们能够将输入文本分类为消极或积极的情绪。
该算法的流程图大致是：
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/20_natural_language_flowchart.png)

# 递归神经网络
递归神经网络（RNN）中的基本构件是递归单元（RU）。经常性单元有许多不同的变体，例如相当笨重的LSTM（长期短期记忆）和稍微简单的GRU（门控循环单元），我们将在本教程中使用它。文献中的实验表明，LSTM和GRU具有大致相似的性能。甚至更简单的变体也存在，并且文献表明它们可能比LSTM和GRU表现得更好，但是它们不在Keras中实现，我们将在本教程中使用它们。

下图显示了经常性单位的抽象概念，该单位具有每次单位接收新输入时都会更新的内部状态。这个内部状态用作一种记忆。但是，它并不是传统类型的存储可以打开或关闭位的计算机内存。相反，经常性单元将浮点值存储在其存储器状态中，这些浮点值是使用矩阵操作读取和写入的，因此操作都是可微分的。这意味着内存状态可以存储任意浮点值（虽然通常限制在-1.0和1.0之间），并且可以像使用渐变下降的正常神经网络一样训练网络。

新的状态值取决于旧的状态值和当前的输入。例如，如果状态值已经记住了我们最近看到“not”这个词并且当前输入是“good”，那么我们需要存储一个新的状态值，它记住了表示负面情绪的“不好”。

负责将旧状态值和输入映射到新状态值的循环单元部分称为门，但它实际上只是一种矩阵操作。计算经常性单位的输出值有另一个门。这些门的执行情况因不同类型的经常性单位而异。这个数字仅仅显示了一个经常性单位的抽象概念。 LSTM比GRU拥有更多的门，但其中一些显然是多余的，因此可以省略。

为了训练循环单元，我们必须逐渐改变门的权重矩阵，以便循环单元为输入序列提供期望的输出。这是在TensorFlow中自动完成的。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/20_recurrent_unit.png)

# 展开网络
另一种可视化和理解递归神经网络的方法是“展开”递归。在这个图中，只有一个表示RU的循环单元，它将在一系列时间步中从输入序列接收一个文本字。

每次新序列开始时，RU的初始记忆状态将由Keras / TensorFlow内部重置为零。

在第一个时间步中，单词“this”被输入到RU中，该RU使用其内部状态（初始化为零）及其门来计算新状态。 RU还使用其他门来计算输出，但这里忽略它，因为它仅在序列的末尾需要输出一种总结。

在第二个时间步中，单词“is”被输入到现在使用刚刚通过查看前一个单词“this”而被更新的内部状态的RU中。

“这就是”这个词没有什么意义，所以RU可能不会看到这些单词在内部状态中保存任何重要内容。但是，当它看到第三个单词“不”时，RU已经知道确定输入文本的总体情绪可能是重要的，因此它需要存储在RU的存储器状态中，以后可以使用它当RU在时间步骤6中看到“良好”这个词时。

最后，当整个序列已经被处理时，RU输出一个值的向量，它总结了它在输入序列中看到的内容。然后，我们使用一个具有Sigmoid激活的完全连接图层来获得0.0到1.0之间的单个值，我们将其解释为情绪为负（接近0.0的值）或正值（接近1.0的值）。

请注意，为了清晰起见，此图不显示从文本字到整数标记和嵌入向量以及输出上完全连接的Sigmoid图的映射。

![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/20_unrolled_flowchart.png)

# 3层展开网络
在本教程中，我们将在下面的“展开”图中使用带有3个表示RU1，RU2和RU3的经常性单位（或层）的递归神经网络。
第一层非常像上面展示的单层RNN的展开图。首先，经常单元RU1的内部状态由Keras / TensorFlow初始化为零。然后单词“this”被输入到RU1并且它更新其内部状态。然后处理下一个单词“是”，等等。但不是在序列结束时输出单个汇总值，而是在每个时间步中使用RU1的输出。这创建了一个新的序列，然后可以用作下一个循环单元RU2的输入。对于第二层重复相同的过程，并且这产生了新的输出序列，然后输入到第三层的经常性单元RU3，其最终输出被传递到完全连接的Sigmoid层，其输出0.0（负情绪）和1.0（积极情绪）。
请注意，为了清楚起见，从该图中省略了文本字到整数标记和嵌入向量的映射。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/20_unrolled_3layers_flowchart.png)

# 爆炸和消失的梯度
为了训练重复单元内的门的权重，我们需要最小化一些损失函数，它测量网络的实际输出与期望输出之间的差异。
从上面的“展开”数据中我们可以看到，对于输入序列中的每个单词，重新获得单位都是递归应用的。这意味着每个时间步骤都应用一次循环门。梯度信号必须从损失函数一直回流到第一次使用循环门。如果递归门的梯度是乘积的，那么我们基本上有一个指数函数。
在本教程中，我们将使用超过500个单词的文本。这意味着RU更新其内部存储器状态的门被递归应用超过500次。如果1.01的梯度与自身相乘500次，那么它将给出大约145的值。如果只有0.99的梯度与其自身相乘500次，那么它给出的值大约为0.007。这些被称为爆炸和消失的渐变。经常出现的乘法的唯一梯度是0和1。

为了避免这些所谓的爆炸和消失梯度，在设计循环单元及其闸门时必须小心。这就是GRU的实际实施更复杂的原因，因为它试图将梯度通过大门发回，而不会发生这种失真。


# 结论
本教程展示了使用具有整数标记和嵌入层的递归神经网络进行自然语言处理（NLP）的基本方法。 这被用来对IMDB的电影评论进行情感分析。 如果正确选择超参数，它工作得相当好。 但重要的是要理解这不是人类对文本的理解。 该系统对文本没有任何真正的理解。 这只是一种巧妙的模式识别方式。


# 练习
这些是一些可能有助于提高TensorFlow技能的练习建议。获得TensorFlow的实践经验对于学习如何正确使用它非常重要。
进行任何更改之前，您可能需要备份此笔记本。

- 运行更多的培训时代。它会提高性能吗？
- 如果您的模型适合训练数据，请尝试在GRU中使用丢失图层和丢失。
- 增加或减少词汇表中的单词数量。这在Tokenizer被初始化时完成。它会影响性能吗？
- 将嵌入向量的大小增加到例如200.它会影响性能吗？
- 尝试改变递归神经网络的所有不同的超参数。
- 使用教程＃19中的贝叶斯优化来找到超参数的最佳选择。
- 使用'post'填充和截断pad_sequences（）。它会影响性能吗？
- 使用单个字符而不是标记化词作为词汇表。然后，您可以对每个字符使用单独编码的矢量，而不是使用嵌入图层。
- 使用model.fit_generator（）而不是model.fit（）并创建自己的数据生成器，该数据生成器使用x_train_tokens的随机子集创建一批数据。序列必须填充，以便它们全部匹配最长序列的长度。
- 向朋友解释程序是如何工作的。
