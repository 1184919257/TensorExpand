参考：

- [19_Hyper-Parameters.ipynb](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb)
- [Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)


----------
# 介绍
在TensorFlow中构建和训练神经网络时，可以选择许多参数。这些通常被称为**超参数**。例如，网络应该有**多少层**有一个超参数，每层有**多少个节点**的另一个超参数，以及**激活函数**使用的另一个超参数等等。**优化方法**也有一个参数或者您可以选择更多的超参数，例如**学习率**。

搜索优秀超参数的一种方法是**通过手动调整**，在那里您尝试一组参数并查看它们如何执行，然后尝试另一组参数并查看它们是否提高了性能。你试着建立一个很好的直觉，并相应地指导你的参数搜索。这不仅对于人类研究人员来说非常耗时，而且对于人类而言，最佳参数通常是违反直觉的，因此您不会找到它们！

另一种搜索良好超参数的方法是**将每个参数的有效范围分成均匀间隔的值**，然后只需让**计算机尝试所有参数值的组合**。这被称为**网格搜索**。虽然它完全由计算机运行，但它会很快变得非常耗时，因为随着您添加更多超参数，参数组合的数量会呈指数增长。这个问题被称为维度的诅咒。例如，如果您只有4个超参数可以调整，并且每个参数都允许10个可能的值，那么总共有10 ^ 4个参数组合。如果再添加一个超参数，则会有10 ^ 5个参数组合，依此类推。

寻找好的超参数的另一种方法是通过**随机搜索**。我们现在不是像在网格搜索中那样系统地尝试每一个参数组合，而是完全随机地尝试一些参数组合。这就像搜索“大海捞针”一样，随着参数数量的增加，通过随机采样找到最优参数组合的概率降为零。

本教程使用一种聪明的方法来寻找称为**贝叶斯优化的良好超参数**。您应该熟悉TensorFlow，Keras和卷积神经网络，请参见教程＃01，＃02和＃03-C。

# 流程图
超参数优化的问题在于**评估一组参数的性能是非常昂贵**的。这是因为我们首先必须建立相应的神经网络，然后我们必须训练它，最后我们必须在测试集上测量它的性能。在本教程中，我们将使用小型MNIST问题，这样可以快速完成此训练，但对于更现实的问题，培训可能需要几小时，几天甚至几周才能完成。因此，我们需要一种可以尽可能高效地搜索超参数的优化方法，只需在绝对必要时评估实际性能。

**贝叶斯优化**的思想是为超参数构建另一个搜索空间模型。一种模型被称为高斯过程。这给我们估计性能如何随着超参数的变化而变化。每当我们评估一组超参数的实际性能时，我们知道一个事实是什么性能 - 除了一些噪声。然后，我们可以要求**贝叶斯优化器**为我们尚未探索的搜索空间区域中的超参数提供新的建议，或者贝叶斯优化器认为的超参数将为我们带来最大的改进。然后，我们重复这个过程，直到贝叶斯优化器建立了一个很好的模型，说明不同超参数的性能如何变化，所以我们可以选择最佳参数。

该算法的流程图大致是：
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/19_flowchart_bayesian_optimization.png)


# 结论
本教程展示了如何使用贝叶斯优化优化神经网络的超参数。我们使用了仍在开发中的scikit-optimize（skopt）库，但它已经是一个非常强大的工具。它能够在很少的迭代中显着改善手动调整的超参数。这比网格搜索和随机搜索超参数要好得多，这将需要更多的计算时间，并且很可能会发现劣质的超参数，特别是对于更难的问题。
# 练习
这些是一些可能有助于提高TensorFlow技能的练习建议。获得TensorFlow的实践经验对于学习如何正确使用它非常重要。
进行任何更改之前，您可能需要备份此笔记本。

- 尝试并运行100或200次迭代的优化，而不是仅仅迭代40次。绘制的景观会发生什么？
- 尝试一些scikit优化的其他优化方法，例如forest_minimize而不是gp_minimize。他们如何执行？
- 尝试使用优化程序的另一个采集函数，例如改进的可能性。
- 尝试使用贝叶斯优化优化更多超参数。例如，卷积层中的内核大小和过滤器数量，或训练中使用的批量大小。
- 为卷积图层的数量添加超参数，并在create_model（）中实现它。请注意，如果您在卷积之后有池化图层，那么图像将进行下采样，因此在图像变得太小之前，您可以拥有的图层数量有限。
- 看看情节。你认为一些超参数可能无关紧要吗？尝试并删除这些参数并重新优化剩余的超参数。
- 在图像文件中使用另一个更难的数据集。
- 增加训练的迭代次数。它是否提高了验证和测试集的分类准确性？它是如何影响时间使用的？
- 向朋友解释程序是如何工作的。
