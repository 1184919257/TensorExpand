[toc]
# 数据说明

- 训练数据 ：
895035张汉字图片，图片大小不一，通道数均为1，共3755个类别
- 测试数据：
223991张汉字图片，图片大小不一，通道数均为1，共3755个类别

查看图片长什么样，以下是其中2张图片：
<center>![这里写图片描述](http://img.blog.csdn.net/20180305111050456?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2M3ODE3MDgyNDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) ![这里写图片描述](http://img.blog.csdn.net/20180305111105931?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2M3ODE3MDgyNDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>

# 系统及硬件环境

```
Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz
CPU 8G
GeForce GTX 960
GPU 4G
Ubuntu 16.04.1
tensorflow-gpu 1.4.1
python 3.5.2
```

# 数据处理
因为图片大小不一致，不能按batch来训练，因此需统一大小，这里将图片resize为32x32，并将图片转成二值化图片，最后转成tfrecord数据(每5W个图片保存成一个tfrecord文件)用于后续加载数据。

部分代码如下，详细代码参考：`data_processing.py`

```python
# image_path 图片路径
img = cv2.imread(image_path, 0)
ret, img = cv2.threshold(img, np.mean(img), 1, cv2.THRESH_BINARY_INV) # 二值化处理 像素值0、1
img = cv2.resize(img, dsize=(img_pixel, img_pixel)) # 缩放到统一大小 img_pixel=32
```
# 模型搭建
输入数据的shape [n,32,32,1]
训练图片数据895035，分类数3755

参考resnet网络结构进行模型搭建
![这里写图片描述](http://img.blog.csdn.net/20180305112627082?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2M3ODE3MDgyNDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
并且使用leaky_relu激活函数，每一卷积层后面加上batch norm层，使得模型更易收敛

模型流程图为：

![这里写图片描述](http://img.blog.csdn.net/20180305155554045?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2M3ODE3MDgyNDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

部分代码如下，详细代码参考：`main.py`

```python
x=tf.placeholder(tf.float32,[None,image_size,image_size])

net=tf.expand_dims(x,-1) # [n,32,32,1]

net=tf.layers.conv2d(net,64,7,1,'same',activation=tf.nn.leaky_relu) # [n,32,32,64]
net=slim.batch_norm(net,is_training=is_training)
net=tf.layers.max_pooling2d(net,2,2,'same') # [n,16,16,64]

branch1=tf.layers.conv2d(net,64,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,16,16,64]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,64,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,16,16,64]
branch2=slim.batch_norm(branch2,is_training=is_training)

net=tf.nn.leaky_relu(net+branch2)
branch1=tf.layers.conv2d(net,64,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,16,16,64]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,64,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,16,16,64]
branch2=slim.batch_norm(branch2,is_training=is_training)

net=tf.nn.leaky_relu(net+branch2)
branch1=tf.layers.conv2d(net,64,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,16,16,64]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,64,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,16,16,64]
branch2=slim.batch_norm(branch2,is_training=is_training)

net=tf.nn.leaky_relu(net+branch2)
net=tf.layers.max_pooling2d(net,2,2,'same') # [n,8,8,64]

branch1=tf.layers.conv2d(net,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch2=slim.batch_norm(branch2,is_training=is_training)
# net=tf.nn.leaky_relu(net+branch2)
net=tf.layers.conv2d(net,128,1,1,'same',activation=tf.nn.leaky_relu) # [n,8,8,128]
net=tf.nn.leaky_relu(net+branch2) # [n,8,8,128]

branch1=tf.layers.conv2d(net,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2) # [n,8,8,128]

branch1=tf.layers.conv2d(net,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2) # [n,8,8,128]

branch1=tf.layers.conv2d(net,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,128,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,8,8,128]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2) # [n,8,8,128]
net=tf.layers.max_pooling2d(net,2,2,'same') # [n,4,4,128]

branch1=tf.layers.conv2d(net,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch2=slim.batch_norm(branch2,is_training=is_training)
# net=tf.nn.leaky_relu(net+branch2)
net=tf.layers.conv2d(net,256,1,1,'same',activation=tf.nn.leaky_relu) # [n,4,4,256]
net=tf.nn.leaky_relu(net+branch2) # [n,4,4,256]

branch1=tf.layers.conv2d(net,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)

branch1=tf.layers.conv2d(net,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)

branch1=tf.layers.conv2d(net,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)

branch1=tf.layers.conv2d(net,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)

branch1=tf.layers.conv2d(net,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,256,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,4,4,256]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)
net=tf.layers.max_pooling2d(net,2,2,'same') # [n,2,2,256]

branch1=tf.layers.conv2d(net,512,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,2,2,512]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,512,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,2,2,512]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.layers.conv2d(net,512,1,1,'same',activation=tf.nn.leaky_relu) # [n,2,2,512]
net=tf.nn.leaky_relu(net+branch2) # [n,2,2,512]
# net=tf.nn.leaky_relu(net+branch2)

branch1=tf.layers.conv2d(net,512,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,2,2,512]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,512,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,2,2,512]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)

branch1=tf.layers.conv2d(net,512,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,2,2,512]
branch1=slim.batch_norm(branch1,is_training=is_training)
branch2=tf.layers.conv2d(branch1,512,3,1,padding='same',activation=tf.nn.leaky_relu) # [n,2,2,512]
branch2=slim.batch_norm(branch2,is_training=is_training)
net=tf.nn.leaky_relu(net+branch2)
net=tf.layers.max_pooling2d(net,2,2,'same') # [n,1,1,512]

# net=tf.squeeze(net) # [n,256]
net=tf.reshape(net,[-1,512])
print(net.shape)
fc1=tf.layers.dense(net,1024,activation=tf.nn.leaky_relu)
fc1=slim.dropout(fc1,keep_rate,is_training=is_training)
prediction=tf.layers.dense(fc1,num_class)
```

# 使用说明
## 1、数据转换
运行以下命令，将训练数据和测试数据转成tfrecord数据

```python
python3 data_processing.py
# 数据保存在`./data`
# ls ./data
# test-0.tfrecords, train-0.tfrecords ...
```
## 2、训练

### 参数说明

```python
# 设置超参数
train_num_images=895035 # 训练样本数
test_num_images=223991 # 测试样本数
num_class=3755 # 分类数
image_size=32 # 样本数据大小
batch_size=args.batch_size # 每步训练的样本数
learning_rate=args.learning_rate # 学习率
global_step = tf.Variable(0, name="global_step", trainable=False)
# learning_rate=tf.train.polynomial_decay(1e-4,global_step,train_num_images*2//batch_size,1e-8)
epochs=args.epochs # 数据迭代次数
train=args.mode # 选择模式 1 train,0 test
keep=args.keep # dropout out保持率
logdir=args.logdir # 模型参数存放位置
```

设置train=1（train=0为测试），
运行
```python
python3 main.py --mode 1
```
### 训练结果


## 3、测试
由于测试数据量很大，因此按batch来测试，最后统计所有batch的精度平均值作为最终的测试精度

运行命令：

```python
python3 main.py --mode 0
```
### 测试结果

# 其他
## 1、模型训练上走了一些弯路
刚开始参考VGG，inception编写模型，通过修改learning_rate，隐藏层数，每一层的节点数，每个卷积层后面加batch norm层，更换激活函数，优化器，经过训练发现效果不好
## 2、模型训练的时间短
由于每个epoch训练完成大概要2~3个小时，而平时时间也不多，因此真正训练的时间不长










