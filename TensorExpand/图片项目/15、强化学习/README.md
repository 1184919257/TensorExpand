参考：

- 1、[16_Reinforcement_Learning.ipynb](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb)
- 2、[Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)


----------
# 介绍
本教程是关于所谓的强化学习，其中一个代理正在学习如何导航一些环境，在这种情况下是1970年代至80年代的Atari游戏。代理人不知道游戏的任何内容，必须学会如何通过试验和错误来玩游戏。代理人可以获得的唯一信息是游戏的屏幕输出，以及前一个操作是否导致奖励或罚款。

这是机器学习/人工智能中的一个非常困难的问题，因为代理必须学会区分游戏图像中的特征，然后将游戏图像中某些特征的出现与其自己的行为以及奖励或惩罚联系起来这可能推迟到未来许多步骤。

Google DeepMind的研究人员首先解决了这个问题。本教程基于他们早期研究论文（尤其是本文和本文）的主要观点，尽管我们做了一些更改，因为原始DeepMind算法在某些方面很笨拙并且过于复杂。但事实证明，为了稳定代理的培训，您仍然需要一些技巧，所以本教程中的实现不幸也有些复杂。

基本思想是**当代理从游戏环境中看到图像时，让代理估计所谓的Q值**。 Q值告诉代理人哪种行为最有可能导致未来最高的累积奖励。然后将问题简化为找到这些Q值并将它们存储起来，以便以后使用函数逼近器进行检索。

这建立在以前的一些教程上。您应该熟悉教程＃01和＃02中的TensorFlow和卷积神经网络。如果您熟悉教程＃03或＃03-B中的某个构建器API，这也会很有帮助。

# 问题
本教程使用Atari游戏Breakout，玩家或代理应该用桨拍击球，从而避免死亡，同时在球击碎墙壁时得分。
当人类学习玩这样的游戏时，首先要弄清楚的是你正在控制的游戏环境的哪一部分 - 在这种情况下是底部的桨。如果您在操纵杆上右移，则桨板向右移动，反之亦然。接下来要弄清楚游戏的目标是什么 - 在这种情况下，要尽可能多地在墙上打砖块以最大限度地提高分数。最后，你需要学习如何避免 - 在这种情况下，你必须避免死亡，让球通过桨。
下面显示了游戏中的3张图片，这些图片展示了我们需要我们的代理商学习的内容在左侧的图像中，球向下，球员必须学会移动球拍才能击球并避免死亡。中间的图像显示了桨击中球，最终导致图像在右侧，球击碎一些砖块和得分。然后球继续向下，重复这个过程。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/16_problem.png)

问题是在球向下和桨击球之间有10个状态，并且还有18个状态，然后在球击中墙壁并打碎一些砖时获得奖励。 我们如何教导一个代理人将这三种情况联系起来并推广到类似的情况？ 如本教程所示，答案是使用所谓的神经网络增强学习。

# Q-Learning

进行强化学习的最简单方法之一就是Q-learning。这里我们要估计所谓的Q值，也称为动作值，因为它们将游戏环境的状态映射为代理可能采取的每种可能动作的数值。 Q值表示预期哪种行为会导致最高的未来报酬，从而告诉代理采取何种行动。

不幸的是，我们不知道Q值应该是什么，所以我们必须以某种方式估计它们。 Q值全部初始化为零，然后在从玩游戏的代理处收集新信息时重复更新。当代理人得分时，Q值必须用新信息更新。

更新Q值有不同的公式，但最简单的方法是将新Q值设置为观察到的奖励，再加上以下游戏状态的最大Q值。这给出了代理人可以从当前游戏状态和以后期望得到的总奖励。通常我们还会将以下状态的最大Q值乘以略低于1的所谓贴现因子。这会导致更远的奖励对Q值贡献较少，从而使代理人更喜欢时间上更接近的奖励。

更新Q值的公式是：
Q-value for state and action = reward + discount * max Q-value for next state

在学术论文中，这通常是用这样的数学符号编写的：
![这里写图片描述](https://render.githubusercontent.com/render/math?math=Q%28s_%7Bt%7D,a_%7Bt%7D%29%20%5Cleftarrow%20%5Cunderbrace%7Br_%7Bt%7D%7D_%7B%5Crm%20reward%7D%20%2b%20%5Cunderbrace%7B%5Cgamma%7D_%7B%5Crm%20discount%7D%20%5Ccdot%20%5Cunderbrace%7B%5Cmax_%7Ba%7DQ%28s_%7Bt%2b1%7D,%20a%29%7D_%7B%5Crm%20estimate~of~future~rewards%7D&mode=display)

此外，当代理人失去生命时，我们知道未来的回报为零，因为代理人已经死亡，所以我们将该状态的Q值设置为零。

# 简单的例子
下面的图片演示了如何通过之前访问过的游戏状态向后扫描Q值。 在这个简单的例子中，我们假设所有的Q值已经初始化为零。 代理在最右侧的图像中获得1分的奖励。 这个奖励然后向后传播到以前的游戏状态，所以当我们在未来看到类似的游戏状态时，我们知道给定的行为导致了这种奖励。
discounting是一种呈指数下降的函数。 这个例子使用折扣因子0.97，所以第三张图片的Q值约为 $0.885 \simeq 0.97^4$，因为它是在实际收到奖励的状态之前的4个状态。 对于其他州也是如此。 此示例仅显示每个状态的一个Q值，但实际上状态中每个可能的操作都有一个Q值，并且使用上面的公式更新Q值以反向扫描。 这将在下一节中介绍。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/16_q-values-simple.png)

# 详细示例
这是一个更详细的例子，显示了游戏环境的两个连续状态的Q值以及如何更新它们。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/16_q-values-details.png)

可能动作的Q值已通过神经网络进行了估计。 对于状态t中的动作NOOP，估计Q值为2.900，这是该状态的最高Q值，因此代理采取该行动，即代理在状态t和t + 1之间不做任何事情，因为NOOP 意味着“无操作”。
在状态t + 1中，代理人得分为4分，但在此实施中限制为1分，以稳定训练。 状态t + 1的最大Q值为RIGHTFIRE动作的1.830。 因此，如果我们选择该行动并继续选择由神经网络估计的Q值提出的行动，则所有未来奖励的折扣总额预计为1.830。
现在我们已知道从NO到t + 1的NOOP行动的回报，我们可以更新Q值来包含这些新信息。 这使用上面的公式：
![这里写图片描述](https://render.githubusercontent.com/render/math?math=Q%28state_%7Bt%7D,NOOP%29%20%5Cleftarrow%20%5Cunderbrace%7Br_%7Bt%7D%7D_%7B%5Crm%20reward%7D%20%2b%20%5Cunderbrace%7B%5Cgamma%7D_%7B%5Crm%20discount%7D%20%5Ccdot%20%5Cunderbrace%7B%5Cmax_%7Ba%7DQ%28state_%7Bt%2b1%7D,%20a%29%7D_%7B%5Crm%20estimate~of~future~rewards%7D%20=%201.0%20%2b%200.97%20%5Ccdot%201.830%20%5Csimeq%202.775&mode=display)

新的Q值为2.775，略低于先前估计的2.900。 这个神经网络已经训练了150个小时，所以它在估计Q值方面非常好，但是在训练之前，估计的Q值会更加不同。
这个想法是让代理人玩很多很多的游戏，并重复更新Q值的估计值，因为有更多关于奖励和惩罚的信息可用。 如果训练在数值上是稳定的，这最终将导致对Q值的良好估计，如下面进一步讨论的。 通过这样做，我们在奖励和以前的行动之间建立了联系。

# 动作追踪
如果我们只使用来自游戏环境的单个图像，那么我们无法分辨球正在移动的方向。 典型的解决方案是使用多个连续的图像来表示游戏环境的状态。
该实现使用另一种方法，通过在运动跟踪器中处理来自游戏环境的图像，输出两个图像，如下所示。 左侧图像来自游戏环境，右侧图像是经过处理的图像，显示了游戏环境中最近移动的痕迹。 在这种情况下，我们可以看到球正在向下移动并从右侧壁弹开，并且球拍从屏幕的左侧移动到右侧。
请注意，动作追踪器仅经过了Breakout测试并部分测试过太空侵略者，因此它可能不适用于具有更复杂图形的游戏，如Doom。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/16_motion-trace.png)

# 训练稳定性
我们需要一个函数逼近器，它可以将游戏环境的状态作为输入，并产生该状态的Q值的估计值作为输出。 我们将为此使用卷积神经网络。 尽管近年来他们已经取得了很高的声誉，但他们实际上是一个相当古老的技术，存在很多问题 - 其中之一就是训练稳定性。 本教程的重要部分研究花费在调整和稳定神经网络的训练上。
要理解为什么训练稳定性是一个问题，请考虑下面的3个图像，它们以3个连续状态显示游戏环境。 在状态$ t $，代理即将得分，发生在以下状态$ t + 1 $中。 假设在此之前所有Q值都为零，现在我们应该将状态$ t + 1 $的Q值设置为1.0，并且如果折扣值为0.97，则状态$ t $应该为0.97，根据 以上公式用于更新Q值。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/16_training_stability.png)

如果我们要训练一个神经网络来估计Q值分别为0.97和1.0的两个状态$ t $和$ t + 1 $的Q值，则神经网络很可能无法正确区分 这两个国家的形象。 因此，神经网络还将估计状态$ t + 2 $的接近1.0的Q值，因为图像非常相似。 但是这显然是错误的，因为状态$ t + 2 $的Q值应该为零，因为我们对此时的未来奖励一无所知，这就是Q值应该估计的。
如果继续这样做，并且在观察到每个新的游戏状态之后对神经网络进行训练，那么它将很快导致估计的Q值爆炸。 这是训练神经网络的人工产物，它必须具有足够大和多样化的训练集。 出于这个原因，我们将使用所谓的重放记忆，这样我们就可以收集大量的游戏状态并在训练神经网络时对其进行洗牌。

# 流程图
该流程图大致显示了本教程中如何实施钢筋学习。有两个主循环顺序运行，直到神经网络在估计Q值时足够精确。
第一个循环用于玩游戏和记录数据。这使用神经网络来估计来自游戏状态的Q值。然后它将游戏状态以及相应的Q值和奖励/惩罚存储在重放存储器中供以后使用。
当重放存储器充满时，另一个环路被激活。首先它通过Replay Memory进行完全反向扫描，以更新所观察到的新奖励和惩罚更新Q值。然后它执行优化运行，以训练神经网络以更好地估计这些更新的Q值。
实现中还有更多细节，例如降低学习率并增加训练期间使用的重播内存的比例，但此流程图显示了主要思想。
![这里写图片描述](https://github.com/Hvass-Labs/TensorFlow-Tutorials/raw/1284750b1c47fe5c376f249e206350b36a7eb3c8/images/16_flowchart.png)

# 神经网络体系结构
该实现中使用的神经网络具有3个卷积层，所有这些层都具有3×3的滤波器尺寸。这些图层分别具有16,32和64个输出通道。前两个卷积层的步幅为2，最后一层的步幅为1。
在3个卷积层之后，有4个完全连接的层，每层有1024个单元和ReLU激活。然后有一个线性激活的完全连接层用作神经网络的输出。
这种架构与DeepMind和其他研究论文中通常使用的架构不同。它们通常具有8x8和4x4的大卷积滤波器尺寸以及高步幅值。这会导致对游戏状态图像进行更积极的下采样。它们通常也只有一个具有256或512个ReLU单元的完全连接层。
在本教程的研究过程中，发现卷积层中较小的滤波器尺寸和跨度以及多个具有更多单元的完全连接层是必要的，以便具有足够精确的Q值。 DeepMind最初使用的神经网络架构似乎显着扭曲了Q值。他们的方法仍然有效的原因可能是由于他们使用了一个拥有100万个状态的非常大的Replay Memory，并且神经网络为游戏环境的每一步做了一小批培训，还有一些其他技巧。
这里使用的体系结构可能过多，但需要数天的培训才能测试每个体系结构，因此作为练习，读者可以尝试找到一个性能较好的较小的神经网络体系结构。




